# -*- coding: utf-8 -*-
"""Adversarially Learned One-Class Classifier for Novelty Detection (ALOCC)
 https://openaccess.thecvf.com/content_cvpr_2018/papers/Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper.pdf
 Note, that this is another implementation of ALOCC as the one from https://github.com/khalooei/ALOCC-CVPR2018
"""
# Author: Michiel Bongaerts (but not author of ALOCC method)
# License: BSD 2 clause

from __future__ import division
from __future__ import print_function

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.utils import check_array
from sklearn.utils.validation import check_is_fitted

from ..utils.utility import check_parameter

from .base import BaseDetector
from .base_dl import _get_tensorflow_version

import matplotlib.pyplot as plt


# if tensorflow 2, import from tf directly
if _get_tensorflow_version() == 1:
    raise NotImplementedError('Model not implemented for Tensorflow version 1')

else:
    import tensorflow as tf
    from tensorflow.keras.models import Model
    from tensorflow.keras.layers import (Input, Dense, Layer, Dropout)
    from tensorflow.keras.optimizers import Adam



class ALOCC(BaseDetector):
    """Adversarially Learned One-Class Classifier for Novelty Detection (ALOCC)

    Parameters
    ----------

    output_activation : str, optional (default=None)
        Activation function to use for output layer.
        See https://keras.io/activations/


    activation_hidden : str, optional (default='tanh')
        Activation function to use for output layer.
        See https://keras.io/activations/

    epochs : int, optional (default=500)
        Number of epochs to train the model.

    batch_size : int, optional (default=32)
        Number of samples per gradient update.

    dropout_rate : float in (0., 1), optional (default=0.2)
        The dropout to be used across all layers.

    G_layers : list, optional (default=[20,10,3,10,20])
        List that indicates the number of nodes per hidden layer for the generator.
        Thus, [10,10] indicates 2 hidden layers having each 10 nodes.

    D_layers : list, optional (default=[20,10,5])
        List that indicates the number of nodes per hidden layer for the discriminator.
        Thus, [10,10] indicates 2 hidden layers having each 10 nodes.

    train_noise_std : float in (0., 1), optional (default=0.1)
        Normal noise is added  during training for the generator network, train_noise_std is 
        the standard deviation of this normal noise.

    lambda_recon : float, optional (default=0.4)


    preprocessing : bool, optional (default=True)
        If True, apply standardization on the data.

    verbose : int, optional (default=1)
        Verbosity mode.
        - 0 = silent
        - 1 = progress bar


    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set, i.e.
        the proportion of outliers in the data set. When fitting this is used
        to define the threshold on the decision function.

    Attributes
    ----------

    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data [0,1].
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is
        fitted.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    def __init__(self, activation_hidden = 'tanh', dropout_rate = 0.2,
                 G_layers = [20,10,3,10,20], verbose = 1,
                 D_layers = [20,10,5], epochs = 500, train_noise_std = 0.1,
                 lambda_recon = 0.4, preprocessing = False, learning_rate = 0.0001,
                 batch_size = 32, output_activation = None, contamination = 0.1 ):
        super(ALOCC, self).__init__(contamination = contamination)

        self.activation_hidden = activation_hidden
        self.dropout_rate = dropout_rate
        self.G_layers = G_layers
        self.D_layers = D_layers
        self.output_activation = output_activation
        self.contamination = contamination
        self.epochs = epochs
        self.train_noise_std = train_noise_std
        self.lambda_recon = lambda_recon
        self.learning_rate = learning_rate
        self.preprocessing = preprocessing
        self.batch_size = batch_size
        self.verbose = verbose

        check_parameter(dropout_rate, 0, 1, param_name='dropout_rate', include_left=True)

    def _build_model(self):
        #### Generator #####
        G_in  = Input(shape= (self.n_features_, ), name = 'I1' )
        G_1 = Dropout(self.dropout_rate, input_shape=(self.n_features_,))(G_in)
        last_layer = G_1
        
        G_hl_dict = {}
        for i,l_dim in enumerate(self.G_layers):
            layer_name = 'hl_{}'.format(i)
            G_hl_dict[layer_name] = Dropout(self.dropout_rate)(Dense( l_dim, activation = self.activation_hidden )(last_layer))
            last_layer = G_hl_dict[layer_name]
            
        G_out = Dense(self.n_features_, activation = self.output_activation)(last_layer)

        self.generator = Model(inputs= (G_in) , outputs= [G_out])
        self.hist_loss_generator = []
        self.hist_recon_loss_generator = []
       
        
        
        #### Discriminator #####
        D_in  = Input(shape= (self.n_features_, ), name = 'I1' )
        D_1 = Dropout(self.dropout_rate, input_shape=(self.n_features_,))(D_in)
        last_layer = D_1
        
        D_hl_dict = {}
        for i,l_dim in enumerate(self.D_layers):
            layer_name = 'hl_{}'.format(i)
            D_hl_dict[layer_name] = Dropout(self.dropout_rate)(Dense( l_dim, activation = self.activation_hidden )(last_layer))
            last_layer = D_hl_dict[layer_name]
        
        classifier_node = Dense(1, activation =  'sigmoid' )(last_layer)

        self.discriminator = Model(inputs= (D_in) , outputs= [classifier_node])
        self.hist_loss_discriminator = []

        self.set_lambda_recon(self.lambda_recon)

        # Set optimizer
        opt = Adam( learning_rate = self.learning_rate)  
        self.generator.compile( optimizer= opt)
        self.discriminator.compile( optimizer= opt)


            
    def set_lambda_recon(self, x):
        self.lambda_recon = x


    def plot_learning_curves(self, start_ind = 0, window_smoothening = 10 ):
        fig = plt.figure( figsize=(20,5))

        l_gen = pd.Series( self.hist_loss_generator[start_ind:]).rolling(window_smoothening).mean()
        l_recon = pd.Series( self.hist_recon_loss_generator[start_ind:]).rolling(window_smoothening).mean()
        l_disc = pd.Series( self.hist_loss_discriminator[start_ind:]).rolling(window_smoothening).mean()


        ax = fig.add_subplot(1,3,1)
        ax.plot(range(len(l_gen)),l_gen, )
        ax.set_title('Generator')
        ax.set_ylabel('Loss')
        ax.set_xlabel('Iter')
    
        ax = fig.add_subplot(1,3,2)
        ax.plot(range(len(l_disc)),l_disc )
        ax.set_title('Discriminator')
        ax.set_ylabel('Loss')
        ax.set_xlabel('Iter')

        ax = fig.add_subplot(1,3,3)
        ax.plot(range(len(l_recon)),l_recon)
        ax.set_title('Reconstruction')
        ax.set_ylabel('Loss')
        ax.set_xlabel('Iter')



    def train_step(self, data ):
        cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)
        X_original, X_original_corrupted = data
        
        assert( X_original.shape[0] == X_original_corrupted.shape[0])
        

        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:

            reconstruction = self.generator({'I1':X_original_corrupted}, training=True)

            real_output = self.discriminator({'I1': X_original }, training=True)
            fake_output = self.discriminator({'I1': reconstruction }, training=True)
                                             
                                             
            # Losses for generator
            err_squared =   (X_original - reconstruction)**2
            loss_recon = tf.keras.backend.mean( tf.keras.backend.mean( err_squared, axis=-1 ) ) 
                           
            # Correctly predicted
            loss_discriminator = cross_entropy(tf.ones_like(fake_output), fake_output)
            total_loss_generator =  loss_discriminator +  self.lambda_recon * loss_recon 
               

            ## Losses discriminator
            # 0 == 'fake' and 1 == 'real'
            real_loss = cross_entropy( tf.ones_like(real_output,dtype = 'float32') * 0.9, real_output) #one-sided label smoothening
            fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
            total_loss_discriminator = real_loss + fake_loss
        
            
        
        # Compute gradients
        gradients_gen = gen_tape.gradient(total_loss_generator, self.generator.trainable_variables)
        # Update weights
        self.generator.optimizer.apply_gradients(zip(gradients_gen, self.generator.trainable_variables))
      

        # Compute gradients
        gradients_disc = disc_tape.gradient(total_loss_discriminator, self.discriminator.trainable_variables)
        # Update weights
        self.discriminator.optimizer.apply_gradients(zip(gradients_disc, self.discriminator.trainable_variables))

        
        self.hist_loss_generator.append( np.float32(total_loss_generator.numpy()) )
        self.hist_recon_loss_generator.append( np.float32(loss_recon.numpy()) )
        self.hist_loss_discriminator.append( np.float32(total_loss_discriminator.numpy()) )




    def fit(self, X, y=None):
        """Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """
        # validate inputs X and y (optional)
        X = check_array(X)
        self._set_n_classes(y)

        # Verify and construct the hidden units
        self.n_samples_, self.n_features_ = X.shape[0], X.shape[1]
        self._build_model()

        # Standardize data for better performance
        if self.preprocessing:
            self.scaler_ = StandardScaler()
            X_norm = self.scaler_.fit_transform(X)
        else:
            X_norm = np.copy(X)


        for n in range(self.epochs):
            if( (n % 100 == 0) and (n != 0 ) and (self.verbose == 1 ) ):
                print('Train iter:{}'.format(n))

            # Shuffle train 
            np.random.shuffle(X_norm)

            X_train_sel = X_norm[ 0 : min(self.batch_size, self.n_samples_ ), :]
            noise =  np.random.normal(0, self.train_noise_std, size = X_train_sel.shape) 
            X_train_sel_noise = X_train_sel + noise 

            self.train_step( (np.float32( X_train_sel ), 
                              np.float32( X_train_sel_noise ) ) )



        # Predict on X itself and calculate the reconstruction error as
        # the outlier scores. Noted X_norm was shuffled has to recreate
        if self.preprocessing:
            X_norm = self.scaler_.transform(X)

        self.decision_scores_ = 1 - self.discriminator({'I1': self.generator({'I1':X_norm})}, training=False).numpy()[:,0]
        self._process_decision_scores()
        return self



    def decision_function(self, X):
        """Predict raw anomaly score of X using the fitted detector.

        The anomaly score of an input sample is computed based on different
        detector algorithms. For consistency, outliers are assigned with
        larger anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The training input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """
        check_is_fitted(self, ['decision_scores_'])
        X = check_array(X)

        if self.preprocessing:
            X_norm = self.scaler_.transform(X)
        else:
            X_norm = np.copy(X)

        # Predict on X 
        pred_scores = 1 - self.discriminator({'I1': self.generator({'I1':X_norm})}, training=False).numpy()[:,0]
        return pred_scores
